# Betha

Betha is a tribute to [Alpa](https://github.com/alpa-projects/alpa)

While Alpha is the nuclear weapon for large model training and serving, Betha is a toy example of model parallel training of a simple GPT-J implementation using manual sharding and Ray core.
